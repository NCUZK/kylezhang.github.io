---
layout:     post
title:     	吴恩达 《优化深度神经网络》（一）
subtitle:   2019-07-19 笔记
date:       2019-07-19
author:     koko
header-img: img/post-bg-universe.jpg
catalog: true
catalog: true
tags:

- 深度学习
- deeplearning.ai
---



[TOC]

# 《优化深度神经网络》（一）

[TOC]



##  **Train/Dev/Test sets**

- 数据分成三个部分：Train/Dev/Test sets。有时候可以只有Train/Dev。

- 训练样本和测试样本尽量来自同一分布。

- 通常设置Train sets和Test sets的数量比例为70%和30%。

- 对于大数据样本，Train/Dev/Test sets的比例通常可以设置为98%/1%/1%，或者99%/0.5%/0.5%。样本数据量越大，相应的Dev/Test sets的比例可以设置的越低一些。

## **Bias/Variance**

- 偏差/方差代表欠拟合/过拟合
- 一般来说，Train set error体现了是否出现bias，Dev set error体现了是否出现variance（正确地说，应该是Dev set error与Train set error的相对差值）

这一块可以再次理解下怎么通过这两个error来计算偏差和方差的思想，train error高的话，说明模型没学习到位，肯定出现了高bias，出现了欠拟合。如果train error 和 dev error相对差值高的话，说明高variance，模型在train set上表现更好，在dev set表现更差，出现了过拟合。

## **Basic Recipe for Machine Learning**

### 防止欠拟合和过拟合的方法

- 降低high bias的方法（欠拟合）：
  - 增加神经网络的隐藏层个数、神经元个数
  - 训练时间延长
  - 选择其它更合适的更合适的NN模型
- 降低high variance的方法（过拟合）：
  - 增加训练样本数据（数据增强）
  - 进行正则化Regularization（L1、L2正则化，Dropout)
  - 选择其他更复杂的NN模型
  - Early stoping（提早停止训练网络） 

### 正交化

见笔记

- 解决high bias和high variance的方法是不同的。实际应用中通过Train set error和Dev set error判断是否出现了high bias或者high variance，然后再选择针对性的方法解决问题。
- Bias和Variance的权衡问题：
  - 传统机器学习算法中，Bias和Variance通常是对立的，减小Bias会增加Variance，减小Variance会增加Bias。
  - 深度学习中，通过使用更复杂的神经网络和海量的训练样本，一般能够同时有效减小Bias和Variance。这也是深度学习之所以如此强大的原因之一。

## **Regularization**

### L1、L2正则化的基础概念

- Logistic regression的**L2 regularization**表达式：

$$
\begin{array}{l}{J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m}\|w\|_{2}^{2}} \\ {\|w\|_{2}^{2}=\sum_{j=1}^{n_{x}} w_{j}^{2}=w^{T} w}\end{array}
$$

为什么只对w进行正则化而不对b进行正则化呢？

其实也可以对b进行正则化。但是一般w的维度很大，而b只是一个常数。相比较来说，参数很大程度上由w决定，改变b值对整体模型影响较小。b只是众多参数的一个，所以，一般都忽略它。

- Logistic regression的**L1 regularization**表达式

$$
\begin{array}{l}{J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m}\|w\|_{1}} \\ {\|w\|_{1}=\sum_{j=1}^{n_{x}}\left|w_{j}\right|}\end{array}
$$

与L2 regularization相比，L1 regularization得到的w更加稀疏，即很多w为零值。其优点是节约存储空间，因为大部分w为0，但实际上也并没有节约太逗空间。L1 regularization在解决high variance方面比L2 regularization并不更具优势，L1的在微分求导方面比较复杂。所以，一般L2 regularization更加常用。

### L2正则化在深度学习中的具体应用

- 深度学习的**L2 regularization**表达式：

$$
\begin{array}{l}{J\left(w^{[1]}, b^{[1]}, \cdots, w^{[L]}, b^{[L]}\right)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)+\frac{\lambda}{2 m} \sum_{l=1}^{L}\left\|w^{[l]}\right\|^{2}} \\ {\left\|w^{[l]}\right\|^{2}=\sum_{i=1}^{n^{[l]}} \sum_{j=1}^{n^{l-1}}\left(w_{i j}^{[l]}\right)^{2}}\end{array}
$$

- $$\lambda$$就是正则化参数， 需要在数据集中验证选择最优的$$\lambda$$。
- 通常把$$\left\|\boldsymbol{w}^{[l]}\right\|^{2}$$称为Frobenius范数，记为$$
  \left\|\boldsymbol{w}^{[l]}\right\|_{F}^{2}$$。范数的计算方法：
$$
\|A\|_{F}=\sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n}\left|a_{i j}\right|^{2}}
$$
  - L2 regularization也被称做weight decay。因为它使得在参数更新的时候，$$\boldsymbol{w}^{[l]}$$的梯度有个增量，导致更新$$\boldsymbol{w}^{[l]}$$的时候减去那个增量，使$$\boldsymbol{w}^{[l]}$$比没有正则化的时候小。

$$
\begin{aligned} w^{[l]} & :=w^{[l]}-\alpha \cdot d w^{l l} \\ &=w^{[l]}-\alpha \cdot\left(d w_{b e f o r e}^{[l]}+\frac{\lambda}{m} w^{[l]}\right) \\ &=\left(1-\alpha \frac{\lambda}{m}\right) w^{[l]}-\alpha \cdot d w_{b e f o r e}^{[l]} \end{aligned}
$$

Remind:$$1-\alpha \frac{\lambda}{m}<1$$

### 正则化如何避免过拟合的？

- 如果使用L2 regularization，当$$\lambda$$很大时，$$\boldsymbol{w}^{[l]} \approx 0$$。意味着该神经网络模型中的某些神经元实际的作用很小，可以忽略。从效果上来看，其实是将某些神经元给忽略掉了。这样原本过于复杂的神经网络模型就变得不那么复杂了，而变得非常简单化了。如下图所示，整个简化的神经网络模型变成了一个逻辑回归模型。问题就从high variance变成了high bias了。

  ![](https://raw.githubusercontent.com/kokozeng/blog/master/image/20190710163251.png)
	因此，选择合适大小的$$\lambda$$值，就能够同时避免high bias和high variance，得到最佳模型。

- ![](https://raw.githubusercontent.com/kokozeng/blog/master/image/20190710163501.png)

## **Dropout**

Dropout是指在深度学习网络的训练过程中，对于每层的神经元，按照一定的概率将其暂时从网络中丢弃。也就是说，每次训练时，每一层都有部分神经元不工作，起到简化复杂网络模型的效果，从而避免发生过拟合。


![](https://raw.githubusercontent.com/kokozeng/blog/master/image/20190710185803.png)

### Inverted dropout

https://redstonewill.com/1052/ 具体内容见这个笔记，以后再更新。

### Dropout为什么工作？

见笔记

## **Normalizing inputs**

## **Vanishing and Exploding gradients**

## **Weight Initialization for Deep Networks**

## **Gradient checking**

梯度check没看视频